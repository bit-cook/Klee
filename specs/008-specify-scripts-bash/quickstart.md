# Quickstart: Marketplace Private Mode å¼€å‘æŒ‡å—

**Feature**: Marketplace Private Mode - æœ¬åœ°å¼€æºå¤§æ¨¡å‹ç®¡ç†
**Date**: 2025-10-23
**Target Audience**: å¼€å‘å›¢é˜Ÿæˆå‘˜

## æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©å¼€å‘è€…å¿«é€Ÿä¸Šæ‰‹ Marketplace Private Mode åŠŸèƒ½çš„å¼€å‘ã€‚æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼Œæ‚¨å¯ä»¥åœ¨ 30 åˆ†é’Ÿå†…æ­å»ºå®Œæ•´çš„å¼€å‘ç¯å¢ƒå¹¶ç†è§£æ ¸å¿ƒæ¶æ„ã€‚

---

## 1. å‰ç½®æ¡ä»¶

### 1.1 ç³»ç»Ÿè¦æ±‚

- **Node.js**: v20+ ï¼ˆé¡¹ç›®ä½¿ç”¨ v22.14.0ï¼‰
- **npm**: v9+
- **Ollama**: 0.6.0+ ï¼ˆç³»ç»Ÿå®‰è£…æˆ– electron-ollamaï¼‰
- **ç£ç›˜ç©ºé—´**: è‡³å°‘ 10GB å¯ç”¨ï¼ˆç”¨äºæµ‹è¯•æ¨¡å‹ä¸‹è½½ï¼‰

### 1.2 å®‰è£… Ollamaï¼ˆå¯é€‰ï¼‰

å¦‚æœæ‚¨æƒ³åœ¨å¼€å‘æ—¶ä½¿ç”¨ç³»ç»Ÿ Ollamaï¼š

\`\`\`bash
# macOS (Homebrew)
brew install ollama

# å¯åŠ¨æœåŠ¡
ollama serve

# éªŒè¯
curl http://localhost:11434/api/tags
\`\`\`

**æ³¨æ„**: å¦‚æœä¸å®‰è£…ç³»ç»Ÿ Ollamaï¼Œelectron-ollama ä¼šè‡ªåŠ¨åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ã€‚

---

## 2. é¡¹ç›®è®¾ç½®

### 2.1 å®‰è£…ä¾èµ–

\`\`\`bash
cd /path/to/rafa
npm install

# å®‰è£…å¹¶å‘ä¸‹è½½é˜Ÿåˆ—åº“
npm install p-queue
\`\`\`

### 2.2 å¯åŠ¨å¼€å‘æœåŠ¡å™¨

\`\`\`bash
# å¯åŠ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨
npm run dev

# æˆ–åˆ†åˆ«å¯åŠ¨
npm run client:dev  # Vite + Electron (ç«¯å£ 5173)
npm run server:dev  # Hono API (ç«¯å£ 3000)
\`\`\`

### 2.3 éªŒè¯ç¯å¢ƒ

1. æ‰“å¼€åº”ç”¨ï¼Œåˆ‡æ¢åˆ° **Private Mode**
2. æ‰“å¼€ DevTools (Cmd+Option+I)
3. åœ¨ Console ä¸­è¿è¡Œï¼š
   \`\`\`javascript
   fetch('http://localhost:11434/api/tags').then(r => r.json()).then(console.log)
   \`\`\`
4. åº”è¯¥çœ‹åˆ° Ollama API è¿”å›çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå¦‚æœä¸ºç©ºä¹Ÿæ­£å¸¸ï¼‰

---

## 3. æ ¸å¿ƒæ–‡ä»¶ä½ç½®

### 3.1 é…ç½®æ–‡ä»¶

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| \`client/src/renderer/src/config/models.ts\` | æ¨¡å‹é…ç½®ï¼ˆæœ¬åœ°+äº‘ç«¯ï¼‰ |
| \`client/src/renderer/src/config/local.config.ts\` | Ollama å’Œ Private Mode é…ç½® |

### 3.2 ä¸»è¿›ç¨‹ï¼ˆElectronï¼‰

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| \`client/src/main/local/services/ollama-manager.ts\` | Ollama ç®¡ç†å™¨ï¼ˆå·²å­˜åœ¨ï¼‰ |
| \`client/src/main/local/services/disk-space-manager.ts\` | ç£ç›˜ç©ºé—´æ£€æµ‹ï¼ˆâœ¨ æ–°å¢ï¼‰ |
| \`client/src/main/local/services/ollama-model-manager.ts\` | æ¨¡å‹åˆ é™¤ç®¡ç†ï¼ˆâœ¨ æ–°å¢ï¼‰ |
| \`client/src/main/local/db/queries/models.ts\` | æ¨¡å‹ä½¿ç”¨æŸ¥è¯¢ï¼ˆâœ¨ æ–°å¢ï¼‰ |
| \`client/src/main/ipc/disk-space-handlers.ts\` | ç£ç›˜ç©ºé—´ IPCï¼ˆâœ¨ æ–°å¢ï¼‰ |
| \`client/src/main/ipc/model-handlers.ts\` | æ¨¡å‹ç®¡ç† IPCï¼ˆâœ¨ æ–°å¢ï¼‰ |

### 3.3 æ¸²æŸ“è¿›ç¨‹ï¼ˆReactï¼‰

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| \`client/src/renderer/src/lib/ollama-client.ts\` | Ollama API å®¢æˆ·ç«¯ï¼ˆğŸ”„ æ‰©å±•ï¼‰ |
| \`client/src/renderer/src/lib/queryKeys.ts\` | TanStack Query é”®å·¥å‚ï¼ˆğŸ”„ æ‰©å±•ï¼‰ |
| \`client/src/renderer/src/hooks/ollama-models/\` | æ¨¡å‹ç®¡ç† Hooksï¼ˆâœ¨ æ–°å¢æ¨¡å—ï¼‰ |
| \`client/src/renderer/src/components/marketplace/\` | Marketplace ç»„ä»¶ï¼ˆâœ¨ æ–°å¢ï¼‰ |
| \`client/src/renderer/src/routes/_authenticated/marketplace.index.tsx\` | Marketplace è·¯ç”±ï¼ˆğŸ”„ æ‰©å±•ï¼‰ |

---

## 4. å¼€å‘æµç¨‹

### 4.1 Phase 1: æ¨¡å‹é…ç½®ï¼ˆ5 åˆ†é’Ÿï¼‰

**ç›®æ ‡**: åœ¨ marketplace ä¸­æ˜¾ç¤ºå¯ä¸‹è½½çš„æœ¬åœ°æ¨¡å‹åˆ—è¡¨ã€‚

**æ­¥éª¤**:

1. **æ‰©å±• \`models.ts\`**:
   \`\`\`typescript
   // client/src/renderer/src/config/models.ts
   
   export interface LocalLLMModel {
     name: string
     model: string
     provider: string
     size: number
     minGPU: string
     updatedAt: string
     description?: string
     tags?: string[]
   }
   
   export const localLLMModels: LocalLLMModel[] = [
     {
       name: 'Llama 3.2 1B',
       model: 'llama3.2:1b',
       provider: 'Meta',
       size: 1.3,
       minGPU: 'None',
       updatedAt: '2024-09-25',
       description: 'Ultra-lightweight, best compatibility',
       tags: ['Recommended', 'Fastest'],
     },
     // ... æ·»åŠ æ›´å¤šæ¨¡å‹
   ]
   \`\`\`

2. **åœ¨ marketplace ä¸­æ˜¾ç¤º**:
   \`\`\`typescript
   // client/src/renderer/src/routes/_authenticated/marketplace.index.tsx
   
   import { localLLMModels } from '@/config/models'
   
   function MarketplaceContent() {
     const { isPrivateMode } = useMode()
     
     return (
       <Tabs defaultValue="agents">
         <TabsList>
           <TabsTrigger value="agents">Agents</TabsTrigger>
           <TabsTrigger value="knowledge-bases">Knowledge Bases</TabsTrigger>
           {isPrivateMode && <TabsTrigger value="local-llms">Local LLMs</TabsTrigger>}
         </TabsList>
         
         <TabsContent value="local-llms">
           <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
             {localLLMModels.map(model => (
               <Card key={model.model}>
                 <CardHeader>
                   <CardTitle>{model.name}</CardTitle>
                   <CardDescription>by {model.provider}</CardDescription>
                 </CardHeader>
                 <CardContent>
                   <p>{model.description}</p>
                   <p className="text-sm text-muted-foreground">
                     Size: {model.size} GB | GPU: {model.minGPU}
                   </p>
                 </CardContent>
               </Card>
             ))}
           </div>
         </TabsContent>
       </Tabs>
     )
   }
   \`\`\`

**éªŒè¯**: åˆ‡æ¢åˆ° Private Modeï¼Œè®¿é—® marketplaceï¼Œåº”è¯¥çœ‹åˆ° "Local LLMs" æ ‡ç­¾é¡µã€‚

---

### 4.2 Phase 2: æ¨¡å‹ä¸‹è½½ï¼ˆ20 åˆ†é’Ÿï¼‰

**ç›®æ ‡**: å®ç°æ¨¡å‹ä¸‹è½½åŠŸèƒ½ï¼ˆæ”¯æŒå®æ—¶è¿›åº¦ï¼‰ã€‚

**æ­¥éª¤**:

1. **æ‰©å±• \`ollama-client.ts\`**:
   \`\`\`typescript
   // client/src/renderer/src/lib/ollama-client.ts
   
   export interface OllamaDownloadProgress {
     status: string
     total?: number
     completed?: number
     percent: number
   }
   
   export async function pullOllamaModel(
     modelName: string,
     onProgress: (progress: OllamaDownloadProgress) => void,
     signal?: AbortSignal
   ): Promise<void> {
     const response = await fetch('http://localhost:11434/api/pull', {
       method: 'POST',
       headers: { 'Content-Type': 'application/json' },
       body: JSON.stringify({ name: modelName, stream: true }),
       signal,
     })
     
     const reader = response.body?.getReader()
     const decoder = new TextDecoder()
     let buffer = ''
     
     while (true) {
       const { done, value } = await reader.read()
       if (done) break
       
       buffer += decoder.decode(value, { stream: true })
       const lines = buffer.split('\n')
       buffer = lines.pop() || ''
       
       for (const line of lines) {
         if (!line.trim()) continue
         const data = JSON.parse(line)
         
         onProgress({
           status: data.status,
           total: data.total,
           completed: data.completed,
           percent: data.total && data.completed 
             ? Math.round((data.completed / data.total) * 100) 
             : 0,
         })
       }
     }
   }
   \`\`\`

2. **åˆ›å»ºä¸‹è½½ Hook**:
   \`\`\`typescript
   // client/src/renderer/src/hooks/ollama-models/mutations/useDownloadModel.ts
   
   import { useMutation } from '@tanstack/react-query'
   import { useState, useCallback, useRef } from 'react'
   import { pullOllamaModel } from '@/lib/ollama-client'
   
   export function useDownloadOllamaModel() {
     const [progress, setProgress] = useState<OllamaDownloadProgress | null>(null)
     const abortControllerRef = useRef<AbortController | null>(null)
     
     const mutation = useMutation({
       mutationFn: async (modelName: string) => {
         const controller = new AbortController()
         abortControllerRef.current = controller
         
         try {
           await pullOllamaModel(modelName, setProgress, controller.signal)
         } finally {
           abortControllerRef.current = null
         }
       },
       
       onSuccess: () => {
         setProgress({ status: 'success', percent: 100 })
       },
       
       onError: () => {
         setProgress(null)
       },
     })
     
     const cancel = useCallback(() => {
       abortControllerRef.current?.abort()
       setProgress(null)
     }, [])
     
     return {
       download: mutation.mutate,
       isDownloading: mutation.isLoading,
       progress,
       cancel,
     }
   }
   \`\`\`

3. **åœ¨ UI ä¸­ä½¿ç”¨**:
   \`\`\`typescript
   function LocalLLMCard({ model }: { model: LocalLLMModel }) {
     const { download, isDownloading, progress, cancel } = useDownloadOllamaModel()
     
     return (
       <Card>
         <CardHeader>
           <CardTitle>{model.name}</CardTitle>
         </CardHeader>
         <CardContent>
           {!isDownloading && (
             <Button onClick={() => download(model.model)}>
               Download
             </Button>
           )}
           
           {isDownloading && progress && (
             <div>
               <Progress value={progress.percent} />
               <p>{progress.status}: {progress.percent}%</p>
               <Button onClick={cancel}>Cancel</Button>
             </div>
           )}
         </CardContent>
       </Card>
     )
   }
   \`\`\`

**éªŒè¯**: ç‚¹å‡» "Download" æŒ‰é’®ï¼Œåº”è¯¥çœ‹åˆ°å®æ—¶è¿›åº¦æ¡ã€‚

---

### 4.3 Phase 3: æ¨¡å‹åˆ é™¤ï¼ˆ15 åˆ†é’Ÿï¼‰

**ç›®æ ‡**: å®ç°å®‰å…¨åˆ é™¤æ¨¡å‹ï¼ˆå¸¦ä½¿ç”¨æ£€æµ‹ï¼‰ã€‚

**æ­¥éª¤**:

1. **ä¸»è¿›ç¨‹æ•°æ®åº“æŸ¥è¯¢**:
   \`\`\`typescript
   // client/src/main/local/db/queries/models.ts
   
   import { eq } from 'drizzle-orm'
   import { localChatSessions } from '../schema'
   
   export async function isModelInUse(db, modelId: string): Promise<boolean> {
     const sessions = await db
       .select({ id: localChatSessions.id })
       .from(localChatSessions)
       .where(eq(localChatSessions.model, modelId))
       .limit(1)
     
     return sessions.length > 0
   }
   \`\`\`

2. **IPC å¤„ç†å™¨**:
   \`\`\`typescript
   // client/src/main/ipc/model-handlers.ts
   
   import { ipcMain } from 'electron'
   import { Ollama } from 'ollama'
   import { isModelInUse } from '../local/db/queries/models'
   import { dbManager } from '../local/db/connection-manager'
   
   const ollama = new Ollama({ host: 'http://localhost:11434' })
   
   export function initModelHandlers() {
     ipcMain.handle('model:delete', async (event, modelId: string) => {
       try {
         // æ£€æŸ¥ä½¿ç”¨
         const db = dbManager.getDb()
         const inUse = await isModelInUse(db, modelId)
         
         if (inUse) {
           return {
             success: false,
             error: 'Model is in use by chat sessions',
           }
         }
         
         // åˆ é™¤
         await ollama.delete({ model: modelId })
         
         return { success: true }
       } catch (error) {
         return {
           success: false,
           error: error.message,
         }
       }
     })
   }
   \`\`\`

3. **å‰ç«¯ Mutation**:
   \`\`\`typescript
   // client/src/renderer/src/hooks/ollama-models/mutations/useDeleteModel.ts
   
   import { useMutation, useQueryClient } from '@tanstack/react-query'
   import { toast } from 'sonner'
   
   export function useDeleteModel() {
     const queryClient = useQueryClient()
     
     return useMutation({
       mutationFn: async (modelId: string) => {
         const result = await window.api.model.delete(modelId)
         
         if (!result.success) {
           throw new Error(result.error)
         }
         
         return result
       },
       
       onSuccess: () => {
         toast.success('Model deleted successfully')
         queryClient.invalidateQueries(['local-models'])
       },
       
       onError: (error: Error) => {
         toast.error('Failed to delete model', {
           description: error.message,
         })
       },
     })
   }
   \`\`\`

**éªŒè¯**: å°è¯•åˆ é™¤ä¸€ä¸ªæœªä½¿ç”¨çš„æ¨¡å‹ï¼Œåº”è¯¥æˆåŠŸï¼›å°è¯•åˆ é™¤æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹ï¼Œåº”è¯¥çœ‹åˆ°é”™è¯¯æç¤ºã€‚

---

## 5. è°ƒè¯•æŠ€å·§

### 5.1 æŸ¥çœ‹ Ollama API è¯·æ±‚

åœ¨ DevTools Console ä¸­ï¼š

\`\`\`javascript
// æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
fetch('http://localhost:11434/api/tags')
  .then(r => r.json())
  .then(console.log)

// ç›‘æ§ä¸‹è½½è¿›åº¦
const response = await fetch('http://localhost:11434/api/pull', {
  method: 'POST',
  body: JSON.stringify({ name: 'llama3.2:1b', stream: true }),
})

const reader = response.body.getReader()
const decoder = new TextDecoder()

while (true) {
  const { done, value } = await reader.read()
  if (done) break
  console.log(decoder.decode(value))
}
\`\`\`

### 5.2 æŸ¥çœ‹ TanStack Query ç¼“å­˜

1. æ‰“å¼€ TanStack Query DevToolsï¼ˆåº”ç”¨å³ä¸‹è§’æµ®åŠ¨å›¾æ ‡ï¼‰
2. æŸ¥çœ‹ \`['local-models']\` æŸ¥è¯¢é”®çš„æ•°æ®
3. æ‰‹åŠ¨è§¦å‘ \`invalidateQueries\` æµ‹è¯•ç¼“å­˜å¤±æ•ˆ

### 5.3 æŸ¥çœ‹ SQLite æ•°æ®åº“

\`\`\`bash
# è¿æ¥åˆ° Private Mode æ•°æ®åº“
sqlite3 ~/Library/Application\\ Support/rafa/userData/local.db

# æŸ¥è¯¢ä½¿ç”¨æ¨¡å‹çš„ä¼šè¯
SELECT id, title, model FROM chat_sessions WHERE model = 'llama3:8b';
\`\`\`

---

## 6. å¸¸è§é—®é¢˜

### Q1: Ollama API è¿”å› 404

**åŸå› **: Ollama æœåŠ¡æœªè¿è¡Œã€‚

**è§£å†³**:
\`\`\`bash
# å¯åŠ¨ Ollama
ollama serve

# æˆ–ç­‰å¾… electron-ollama è‡ªåŠ¨å¯åŠ¨ï¼ˆåº”ç”¨å¯åŠ¨åçº¦ 5-10 ç§’ï¼‰
\`\`\`

### Q2: ä¸‹è½½è¿›åº¦ä¸æ›´æ–°

**åŸå› **: NDJSON è§£æé€»è¾‘é”™è¯¯ã€‚

**è°ƒè¯•**:
\`\`\`typescript
// åœ¨ pullOllamaModel ä¸­æ·»åŠ æ—¥å¿—
console.log('[Download] Raw line:', line)
console.log('[Download] Parsed:', JSON.parse(line))
\`\`\`

### Q3: æ¨¡å‹åˆ é™¤å¤±è´¥ï¼š"model is in use"

**åŸå› **: æ¨¡å‹æ­£åœ¨è¢« Ollama è¿è¡Œã€‚

**è§£å†³**:
\`\`\`bash
# åœæ­¢æ¨¡å‹
ollama stop llama3:8b

# ç„¶åé‡è¯•åˆ é™¤
\`\`\`

### Q4: ç£ç›˜ç©ºé—´æ£€æµ‹ä¸å·¥ä½œ

**åŸå› **: Node.js ç‰ˆæœ¬ < 19.6ã€‚

**è§£å†³**: å‡çº§ Node.js åˆ° v20+ï¼Œæˆ–ä½¿ç”¨ \`check-disk-space\` åº“ã€‚

---

## 7. æµ‹è¯•checklist

å¼€å‘å®Œæˆåï¼Œè¿è¡Œä»¥ä¸‹æµ‹è¯•ï¼š

- [ ] æ¨¡å‹åˆ—è¡¨æ˜¾ç¤ºï¼ˆé…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ¨¡å‹ï¼‰
- [ ] æ¨¡å‹ä¸‹è½½ï¼ˆå®æ—¶è¿›åº¦æ¡ï¼‰
- [ ] æ¨¡å‹ä¸‹è½½å–æ¶ˆï¼ˆAbortControllerï¼‰
- [ ] æ¨¡å‹åˆ é™¤ï¼ˆæœªä½¿ç”¨ï¼‰
- [ ] æ¨¡å‹åˆ é™¤ï¼ˆè¢«ä½¿ç”¨ï¼Œæ˜¾ç¤ºè­¦å‘Šï¼‰
- [ ] ç£ç›˜ç©ºé—´æ£€æµ‹ï¼ˆä¸‹è½½å‰æ£€æŸ¥ï¼‰
- [ ] å¹¶å‘ä¸‹è½½é™åˆ¶ï¼ˆæœ€å¤š 2 ä¸ªï¼‰
- [ ] Private Mode éšè— Web Search
- [ ] Cloud Mode ä¸æ˜¾ç¤º Local LLMs æ ‡ç­¾é¡µ
- [ ] æ¨¡å¼åˆ‡æ¢åç¼“å­˜åˆ·æ–°

---

## 8. ä¸‹ä¸€æ­¥

1. é˜…è¯» [research.md](research.md) äº†è§£æŠ€æœ¯å†³ç­–
2. é˜…è¯» [data-model.md](data-model.md) äº†è§£æ•°æ®ç»“æ„
3. è¿è¡Œ \`/speckit.tasks\` ç”Ÿæˆè¯¦ç»†ä»»åŠ¡æ¸…å•
4. å¼€å§‹å®æ–½ï¼

**é¢„è®¡å¼€å‘æ—¶é—´**: 3-5 å¤©ï¼ˆ1 ä½å¼€å‘è€…ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2025-10-23

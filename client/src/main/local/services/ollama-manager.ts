/**
 * OllamaManager - æ™ºèƒ½ Ollama é›†æˆç®¡ç†å™¨
 *
 * åŠŸèƒ½ï¼š
 * - ä¼˜å…ˆæ£€æµ‹å¹¶å¤ç”¨ç³»ç»Ÿ Ollamaï¼ˆé¿å…è¿›ç¨‹å†²çªï¼‰
 * - å¿…è¦æ—¶è‡ªåŠ¨ä¸‹è½½å†…åµŒç‰ˆæœ¬åˆ° userData
 * - ç²¾ç¡®è¿›ç¨‹ç®¡ç†ï¼ˆä¸å½±å“ç”¨æˆ·ç³»ç»Ÿ Ollamaï¼‰
 * - æ”¯æŒè¿›åº¦å›è°ƒå’Œæ¥æºè¿½è¸ª
 */

import { ElectronOllama } from 'electron-ollama'
import { app } from 'electron'
import path from 'path'
import * as fs from 'node:fs/promises'
import { exec } from 'child_process'
import { promisify } from 'util'
import { EMBEDDING_CONFIG } from '../../../../config/local.config'
import {
  detectPlatform,
  EmbeddedPlatform,
  EMBEDDED_OLLAMA_VERSION,
  ensureEmbeddedBinary,
  ensureEmbeddedModels,
  getEmbeddedDataPath,
  getEmbeddedExecutablePath,
} from './ollama-embedded-assets'

const execAsync = promisify(exec)

/**
 * Ollama æ¥æºç±»å‹
 * - system: ç”¨æˆ·å·²å®‰è£…çš„ç³»ç»Ÿ Ollama
 * - embedded: Rafa å†…åµŒçš„ Ollama ç‰ˆæœ¬
 * - none: æœªåˆå§‹åŒ–æˆ–åˆå§‹åŒ–å¤±è´¥
 */
export type OllamaSource = 'system' | 'embedded' | 'none'

/**
 * åˆå§‹åŒ–è¿›åº¦å›è°ƒ
 */
export interface OllamaInitProgress {
  percent: number
  message: string
  source: OllamaSource
}

/**
 * åˆå§‹åŒ–ç»“æœ
 */
export interface OllamaInitResult {
  source: OllamaSource
  url: string
}

export class OllamaManager {
  private ollama: ElectronOllama | null = null
  private readonly basePath: string
  private readonly platform: EmbeddedPlatform
  private ollamaSource: OllamaSource = 'none'
  private readonly ollamaUrl = 'http://localhost:11434'

  constructor() {
    this.basePath = path.join(app.getPath('userData'), 'ollama')
    this.platform = detectPlatform()
  }

  /**
   * T026: åˆå§‹åŒ– Ollamaï¼ˆæ™ºèƒ½æ£€æµ‹ç‰ˆæœ¬ï¼‰
   *
   * ç­–ç•¥ï¼š
   * 1. é¦–å…ˆæ£€æµ‹ç³»ç»Ÿ Ollamaï¼ˆlocalhost:11434ï¼‰
   * 2. å¦‚æœæ£€æµ‹åˆ°ï¼Œç›´æ¥ä½¿ç”¨ç³»ç»Ÿç‰ˆæœ¬
   * 3. å¦‚æœæœªæ£€æµ‹åˆ°ï¼Œä¸‹è½½å¹¶å¯åŠ¨å†…åµŒç‰ˆæœ¬
   *
   * @param onProgress - å¯é€‰çš„è¿›åº¦å›è°ƒå‡½æ•°
   * @returns åˆå§‹åŒ–ç»“æœï¼ˆåŒ…å«æ¥æºå’Œ URLï¼‰
   */
  async initialize(
    onProgress?: (progress: OllamaInitProgress) => void
  ): Promise<OllamaInitResult> {
    try {
      console.log('[OllamaManager] Starting initialization...')
      const ollamaInPath = await this.isOllamaInPath()
      console.log('[OllamaManager] PATH lookup for `ollama`:', ollamaInPath ? 'found' : 'not found')

      // T025.1: æ£€æµ‹ç³»ç»Ÿ Ollama
      const systemOllamaAvailable = await this.detectSystemOllama()

      console.log(`[OllamaManager] System Ollama detection result: ${systemOllamaAvailable}`)

      if (systemOllamaAvailable) {
        this.ollamaSource = 'system'
        console.log('âœ… Using system Ollama at http://localhost:11434')

        // T027: å‘é€è¿›åº¦äº‹ä»¶
        onProgress?.({
          percent: 100,
          message: 'Connected to system Ollama',
          source: 'system',
        })

        return { source: 'system', url: this.ollamaUrl }
      }

      // ç³»ç»Ÿæ—  Ollamaï¼Œä½¿ç”¨å†…åµŒç‰ˆæœ¬
      console.log('ğŸ”„ System Ollama not found, preparing embedded bundle...')
      this.ollamaSource = 'embedded'

      await this.prepareEmbeddedBundle(onProgress)

      this.ollama = new ElectronOllama({ basePath: this.basePath })

      if (!(await this.ollama.isRunning())) {
        console.log('[OllamaManager] Launching embedded Ollama binary...')
        await this.ollama.serve(EMBEDDED_OLLAMA_VERSION, {
          serverLog: (message) => console.log('[EmbeddedOllama]', message),
        })
      }

      console.log('âœ… Embedded Ollama server ready at http://localhost:11434')

      onProgress?.({
        percent: 100,
        message: 'Embedded Ollama ready',
        source: 'embedded',
      })

      return { source: 'embedded', url: this.ollamaUrl }
    } catch (error) {
      console.error('âŒ Failed to initialize Ollama:', error)
      this.ollamaSource = 'none'
      throw error
    }
  }

  /**
   * T025.1: æ£€æµ‹ç³»ç»Ÿæ˜¯å¦å·²è¿è¡Œ Ollama
   *
   * é€šè¿‡æ£€æŸ¥ localhost:11434/api/tags æ˜¯å¦æœ‰ Ollama API å“åº”æ¥åˆ¤æ–­
   *
   * @returns æ˜¯å¦æ£€æµ‹åˆ°ç³»ç»Ÿ Ollama
   */
  private async detectSystemOllama(): Promise<boolean> {
    try {
      console.log('[OllamaManager] Detecting system Ollama at', this.ollamaUrl)

      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), 2000) // 2ç§’è¶…æ—¶

      const response = await fetch(`${this.ollamaUrl}/api/tags`, {
        method: 'GET',
        signal: controller.signal,
      })

      clearTimeout(timeoutId)

      console.log('[OllamaManager] Fetch response status:', response.status)

      if (response.ok) {
        // éªŒè¯è¿”å›çš„æ˜¯ Ollama API å“åº”
        const data = await response.json()
        const hasModels = 'models' in data
        console.log('[OllamaManager] Response has models:', hasModels)
        return hasModels // Ollama /api/tags è¿”å› { models: [...] }
      }

      console.log('[OllamaManager] Response not OK')
      return false
    } catch (error) {
      // ç½‘ç»œé”™è¯¯ã€è¶…æ—¶æˆ–è¿æ¥è¢«æ‹’ç» â†’ ç³»ç»Ÿæ—  Ollama
      console.log('[OllamaManager] Detection failed:', error)
      return false
    }
  }

  /**
   * T025.2: è·å–å½“å‰ä½¿ç”¨çš„ Ollama æ¥æº
   *
   * @returns å½“å‰ Ollama æ¥æº
   */
  getSource(): OllamaSource {
    return this.ollamaSource
  }

  private async prepareEmbeddedBundle(
    onProgress?: (progress: OllamaInitProgress) => void
  ): Promise<void> {
    const log = (message: string) => console.log(message)

    onProgress?.({
      percent: 20,
      message: 'Preparing embedded Ollama binary...',
      source: 'embedded',
    })

    const executablePath = await ensureEmbeddedBinary(this.basePath, this.platform, (message) =>
      log(`[OllamaManager] ${message}`)
    )
    console.log('[OllamaManager] Embedded Ollama binary ready at:', executablePath)

    onProgress?.({
      percent: 40,
      message: 'Preparing bundled embedding models...',
      source: 'embedded',
    })

    await ensureEmbeddedModels(
      this.basePath,
      [EMBEDDING_CONFIG.DEFAULT_MODEL],
      (message) => log(`[OllamaManager] ${message}`)
    )

    const dataPath = getEmbeddedDataPath(this.basePath)
    const modelsPath = path.join(dataPath, 'models')
    const tmpPath = path.join(dataPath, 'tmp')
    console.log('[OllamaManager] Embedded models directory prepared at:', modelsPath)

    await fs.mkdir(tmpPath, { recursive: true })

    if (!process.env.OLLAMA_HOME) {
      process.env.OLLAMA_HOME = dataPath
    }
    if (!process.env.OLLAMA_MODELS) {
      process.env.OLLAMA_MODELS = modelsPath
    }
    if (!process.env.OLLAMA_TMPDIR) {
      process.env.OLLAMA_TMPDIR = tmpPath
    }
    if (!process.env.OLLAMA_HOST) {
      process.env.OLLAMA_HOST = '127.0.0.1:11434'
    }

    process.env.RAFA_EMBEDDED_OLLAMA_HOME = dataPath
    process.env.RAFA_EMBEDDED_OLLAMA_BIN = executablePath

    console.log('[OllamaManager] Embedded environment configured', {
      OLLAMA_HOME: process.env.OLLAMA_HOME,
      OLLAMA_MODELS: process.env.OLLAMA_MODELS,
      OLLAMA_TMPDIR: process.env.OLLAMA_TMPDIR,
      EMBEDDED_BIN: process.env.RAFA_EMBEDDED_OLLAMA_BIN,
    })

    onProgress?.({
      percent: 60,
      message: 'Embedded assets ready, starting server...',
      source: 'embedded',
    })
  }

  private async isOllamaInPath(): Promise<boolean> {
    try {
      if (process.platform === 'win32') {
        await execAsync('where ollama')
      } else {
        await execAsync('command -v ollama')
      }
      return true
    } catch {
      return false
    }
  }

  /**
   * T028 & T028.1: æ™ºèƒ½å…³é—­ Ollama
   *
   * ç­–ç•¥ï¼š
   * - å¦‚æœä½¿ç”¨ç³»ç»Ÿ Ollama â†’ ä¸å…³é—­ï¼ˆé¿å…å½±å“ç”¨æˆ·ï¼‰
   * - å¦‚æœä½¿ç”¨å†…åµŒ Ollama â†’ ç²¾ç¡®å…³é—­ï¼ˆä¸è¯¯æ€ç³»ç»Ÿè¿›ç¨‹ï¼‰
   *
   * å¹³å°ç‰¹å®šé€»è¾‘ï¼š
   * - macOS/Linux: ä½¿ç”¨ pkill -f è¿‡æ»¤ basePath
   * - Windows: ä½¿ç”¨ wmic æŸ¥è¯¢è¿›ç¨‹è·¯å¾„
   */
  async shutdown(): Promise<void> {
    // å¦‚æœä½¿ç”¨ç³»ç»Ÿ Ollamaï¼Œä¸è¦å…³é—­å®ƒ
    if (this.ollamaSource === 'system') {
      console.log('â„¹ï¸ Using system Ollama, skipping shutdown')
      return
    }

    // åªå…³é—­å†…åµŒçš„ Ollama
    if (this.ollamaSource !== 'embedded') {
      return
    }

    const embeddedExecutable = getEmbeddedExecutablePath(this.basePath, this.platform)
    console.log('[OllamaManager] Attempting to stop embedded Ollama', {
      executable: embeddedExecutable,
    })

    try {
      if (process.platform === 'darwin' || process.platform === 'linux') {
        // macOS/Linux: åªæ€æ‰ä» basePath å¯åŠ¨çš„è¿›ç¨‹
        await execAsync(`pkill -f "${embeddedExecutable}"`)
      } else if (process.platform === 'win32') {
        // Windows: é€šè¿‡è¿›ç¨‹è·¯å¾„è¿‡æ»¤
        const escapedPath = embeddedExecutable.replace(/\\/g, '\\\\')
        const findCmd = `wmic process where "ExecutablePath='${escapedPath}'" get ProcessId`

        const { stdout } = await execAsync(findCmd)
        const pids = stdout
          .split('\n')
          .slice(1)
          .map((line) => line.trim())
          .filter(Boolean)

        for (const pid of pids) {
          await execAsync(`taskkill /F /PID ${pid}`)
        }
      }

      console.log('âœ… Embedded Ollama shutdown complete')
    } catch (error) {
      // è¿›ç¨‹å¯èƒ½å·²å…³é—­ï¼Œå¿½ç•¥é”™è¯¯
      console.log('â„¹ï¸ Ollama process already terminated or not found', error)
    }
  }

  /**
   * è·å– Ollama API URL
   */
  getUrl(): string {
    return this.ollamaUrl
  }
}
